// Copyright 2021 Datafuse Labs
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Logs from this module will show up as "[VACUUM] ...".
databend_common_tracing::register_module_tag!("[VACUUM]");

use std::collections::HashSet;
use std::sync::Arc;

use backoff::backoff::Backoff;
use chrono::DateTime;
use chrono::Duration;
use chrono::Utc;
use databend_common_catalog::table::Table;
use databend_common_catalog::table::TableExt;
use databend_common_catalog::table_context::TableContext;
use databend_common_exception::ErrorCode;
use databend_common_exception::Result;
use databend_common_meta_app::schema::SnapshotRefType;
use databend_common_meta_app::schema::TableInfo;
use databend_common_meta_app::schema::UpdateTableMetaReq;
use databend_enterprise_vacuum_handler::VacuumHandlerWrapper;
use databend_meta_types::MatchSeq;
use databend_storages_common_table_meta::meta::Location;
use databend_storages_common_table_meta::meta::VACUUM2_OBJECT_KEY_PREFIX;
use databend_storages_common_table_meta::meta::uuid_from_date_time;
use futures_util::TryStreamExt;
use log::error;
use log::info;
use log::warn;
use opendal::Entry;
use opendal::Operator;
use opendal::Scheme;

use crate::FuseTable;
use crate::io::SnapshotLiteExtended;
use crate::io::SnapshotsIO;
use crate::io::TableMetaLocationGenerator;
use crate::operations::set_backoff;

/// An assumption of the maximum duration from the time the first block is written to the time the
/// snapshot is written.
///
/// To handle the situation during an upgrade where some nodes may not be able to upgrade in time to
/// a version that includes the vacuum2 logic, we introduce this assumption. It is used in two places:
///
/// - When determining whether a snapshot object generated by an old version node can be cleaned up
///
///   Snapshots whose object key does not start with `VACUUM2_OBJECT_KEY_PREFIX` are all created by
///   nodes of previous versions (do not support vacuum2). For such snapshot objects, if their
///   timestamp is less than
///   `GC_root's timestamp - ASSUMPTION_MAX_TXN_DURATION`
///   we consider them safe to delete.
///
///   Generally speaking, if a snapshot from an old version was created a sufficiently long time
///   before the gc root, it would not be successfully committed after the gc root; this way, we
///   avoid deleting a snapshot object produced by an ongoing (not yet committed) transaction.
///
/// - When determining whether a segment/block object generated by an old version query node can be
///   cleaned up
///
///   Similarly, if a segment/block was created at a time sufficiently long before the gc root and
///   is not referenced by the gc root, then it will not be referenced by a snapshot that can be
///   successfully committed after the gc root, and safe to delete.
///
/// NOTE:
///   If this assumption does not hold, it may lead to table data becoming inaccessible:
///   snapshots may become inaccessible, or some data may become unavailable.
///
///   If the entire cluster is upgraded to the new version that includes the vacuum2 logic,
///   the above risks will not exist.
pub const ASSUMPTION_MAX_TXN_DURATION: Duration = Duration::days(3);

/// Object storage supported by Databend is expected to return entries sorted in ascending lexicographical
/// order by object key. Databend leverages this property to enhance the efficiency and thoroughness
/// of the vacuum process.
///
/// The safety of the vacuum algorithm does not depend on this ordering.
async fn general_list_until_prefix(
    dal: &Operator,
    path: &str,
    until: &str,
    need_one_more: bool,
    gc_root_meta_ts: Option<DateTime<Utc>>,
) -> Result<Vec<Entry>> {
    let mut lister = dal.lister(path).await?;
    let mut paths = vec![];
    while let Some(entry) = lister.try_next().await? {
        if entry.metadata().is_dir() {
            continue;
        }
        if entry.path() >= until {
            info!("entry path: {} >= until: {}", entry.path(), until);
            if need_one_more {
                paths.push(entry);
            }
            break;
        }
        if gc_root_meta_ts.is_none()
            || is_gc_candidate_segment_block(&entry, dal, gc_root_meta_ts.unwrap()).await?
        {
            paths.push(entry);
        }
    }
    Ok(paths)
}

/// If storage is backed by FS, we prioritize thoroughness over efficiency (though efficiency loss
/// is usually not significant). All entries are fetched and sorted before extracting the prefix entries.
async fn fs_list_until_prefix(
    dal: &Operator,
    path: &str,
    until: &str,
    need_one_more: bool,
    gc_root_meta_ts: Option<DateTime<Utc>>,
) -> Result<Vec<Entry>> {
    // Fetch ALL entries from the path and sort them by path in lexicographical order.
    let mut lister = dal.lister(path).await?;
    let mut entries = Vec::new();
    while let Some(item) = lister.try_next().await? {
        if item.metadata().is_file() {
            entries.push(item);
        }
    }
    entries.sort_by(|l, r| l.path().cmp(r.path()));

    // Extract entries up to the `until` path, respecting lexicographical order.
    let mut res = Vec::new();
    for entry in entries {
        if entry.path() >= until {
            info!("entry path: {} >= until: {}", entry.path(), until);
            if need_one_more {
                res.push(entry);
            }
            break;
        }
        if gc_root_meta_ts.is_none()
            || is_gc_candidate_segment_block(&entry, dal, gc_root_meta_ts.unwrap()).await?
        {
            res.push(entry);
        }
    }

    Ok(res)
}

/// Check if an entry is a candidate for garbage collection
async fn is_gc_candidate_segment_block(
    entry: &Entry,
    op: &Operator,
    gc_root_meta_ts: DateTime<Utc>,
) -> Result<bool> {
    let path = entry.path();
    let last_part = path.rsplit('/').next().unwrap();
    if last_part.starts_with(VACUUM2_OBJECT_KEY_PREFIX) {
        return Ok(true);
    }
    let last_modified = if let Some(v) = entry.metadata().last_modified() {
        v
    } else {
        let path = entry.path();
        let meta = op.stat(path).await?;
        meta.last_modified().ok_or_else(|| {
            ErrorCode::StorageOther(format!(
                "Failed to get `last_modified` metadata of the entry '{}'",
                path
            ))
        })?
    };

    Ok(last_modified + ASSUMPTION_MAX_TXN_DURATION < gc_root_meta_ts)
}

impl FuseTable {
    pub async fn vacuum_table(
        &self,
        ctx: Arc<dyn TableContext>,
        vacuum_handler: &VacuumHandlerWrapper,
        respect_flash_back: bool,
    ) {
        warn!(
            "Vacuuming table: {}, ident: {}",
            self.table_info.name, self.table_info.ident
        );

        if let Err(e) = vacuum_handler
            .do_vacuum2(self, ctx, respect_flash_back)
            .await
        {
            // Vacuum in a best-effort manner, errors are ignored
            warn!("Vacuum table {} failed : {}", self.table_info.name, e);
        } else {
            info!("Vacuum table {} done", self.table_info.name);
        }
    }

    /// List files until a specific timestamp
    ///
    /// This implementation uses UUID v7 timestamp extraction for precise filtering.
    /// Used by both do_vacuum and do_vacuum2.
    pub async fn list_files_until_timestamp(
        &self,
        path: &str,
        until: DateTime<Utc>,
        need_one_more: bool,
        gc_root_meta_ts: Option<DateTime<Utc>>,
    ) -> Result<Vec<Entry>> {
        let uuid = uuid_from_date_time(until);
        let uuid_str = uuid.simple().to_string();

        // extract the most significant 48 bits, which is 12 characters
        let timestamp_component = &uuid_str[..12];
        let until = format!(
            "{}{}{}",
            path, VACUUM2_OBJECT_KEY_PREFIX, timestamp_component
        );
        self.list_files_until_prefix(path, &until, need_one_more, gc_root_meta_ts)
            .await
    }

    /// List files until a specific location/prefix
    ///
    /// This implementation handles different storage schemes (FS vs object storage) and
    /// includes gc_root_meta_ts checking for safe vacuum operations.
    pub async fn list_files_until_prefix(
        &self,
        path: &str,
        until: &str,
        need_one_more: bool,
        gc_root_meta_ts: Option<DateTime<Utc>>,
    ) -> Result<Vec<Entry>> {
        info!("Listing files until prefix: {}", until);
        let dal = self.get_operator_ref();

        match dal.info().scheme() {
            Scheme::Fs => {
                fs_list_until_prefix(dal, path, until, need_one_more, gc_root_meta_ts).await
            }
            _ => general_list_until_prefix(dal, path, until, need_one_more, gc_root_meta_ts).await,
        }
    }

    /// Collect segments from snapshots in a given prefix
    ///
    /// This is a helper function used by both main branch and branch refs
    async fn collect_snapshots_segments<T>(
        snapshots_io: &SnapshotsIO,
        operator: &Operator,
        snapshot_location: &str,
        root_snapshot_lite: Arc<SnapshotLiteExtended>,
        max_threads: usize,
        segments: &mut HashSet<Location>,
        status_callback: &T,
    ) -> Result<()>
    where
        T: Fn(String),
    {
        // List all the snapshot file paths
        let mut snapshot_files = vec![];
        if let Some(prefix) = SnapshotsIO::get_s3_prefix_from_file(snapshot_location) {
            snapshot_files = SnapshotsIO::list_files(operator.clone(), &prefix, None).await?;
        }

        if snapshot_files.is_empty() {
            return Ok(());
        }

        let start = std::time::Instant::now();
        let mut count = 1;

        // First save root snapshot segments
        root_snapshot_lite.segments.iter().for_each(|location| {
            segments.insert(location.to_owned());
        });

        // Process snapshots in chunks
        for chunk in snapshot_files.chunks(max_threads) {
            // Since we want to get all the snapshot referenced files, so set `ignore_timestamp` true
            let results = snapshots_io
                .read_snapshot_lite_extends(chunk, root_snapshot_lite.clone(), true)
                .await?;

            results
                .into_iter()
                .flatten()
                .for_each(|snapshot_lite_extend| {
                    snapshot_lite_extend.segments.iter().for_each(|location| {
                        segments.insert(location.to_owned());
                    });
                });

            // Refresh status
            count += chunk.len();
            let status = format!(
                "gc orphan: read snapshot files:{}/{}, segment files: {}, cost:{:?}",
                count,
                snapshot_files.len(),
                segments.len(),
                start.elapsed()
            );
            info!("{}", status);
            (status_callback)(status);
        }

        Ok(())
    }

    /// Get all segments referenced by snapshots, including branches and tags
    #[async_backtrace::framed]
    pub async fn get_snapshot_referenced_segments<T>(
        &self,
        ctx: Arc<dyn TableContext>,
        status_callback: T,
    ) -> Result<Option<HashSet<Location>>>
    where
        T: Fn(String),
    {
        // 1. Read the root snapshot
        let root_snapshot_location_op = self.snapshot_loc();
        if root_snapshot_location_op.is_none() {
            return Ok(None);
        }

        let root_snapshot_location = root_snapshot_location_op.unwrap();
        let root_snapshot = match SnapshotsIO::read_snapshot_for_vacuum(
            self.get_operator(),
            root_snapshot_location.as_str(),
        )
        .await?
        {
            Some(snapshot) => snapshot,
            None => return Ok(None),
        };

        let ver = TableMetaLocationGenerator::snapshot_version(root_snapshot_location.as_str());
        let root_snapshot_lite = Arc::new(SnapshotLiteExtended {
            format_version: ver,
            snapshot_id: root_snapshot.snapshot_id,
            timestamp: root_snapshot.timestamp,
            segments: HashSet::from_iter(root_snapshot.segments.clone()),
            table_statistics_location: root_snapshot.table_statistics_location(),
        });
        drop(root_snapshot);

        let snapshots_io = SnapshotsIO::create(ctx.clone(), self.get_operator());
        let operator = self.get_operator();
        let table_info = self.get_table_info();
        let max_threads = ctx.get_settings().get_max_threads()? as usize;

        // 2. Collect segments from main branch
        let mut segments = HashSet::new();
        Self::collect_snapshots_segments(
            &snapshots_io,
            &operator,
            &root_snapshot_location,
            root_snapshot_lite.clone(),
            max_threads,
            &mut segments,
            &status_callback,
        )
        .await?;

        // 3. Collect segments from branches and tags
        for snapshot_ref in table_info.meta.refs.values() {
            match snapshot_ref.typ {
                SnapshotRefType::Tag => {
                    // Read tag snapshot and collect its segments
                    match SnapshotsIO::read_snapshot_for_vacuum(operator.clone(), &snapshot_ref.loc)
                        .await?
                    {
                        Some(snapshot) => {
                            for seg_loc in &snapshot.segments {
                                segments.insert(seg_loc.clone());
                            }
                        }
                        None => {
                            return Ok(None);
                        }
                    }
                }
                SnapshotRefType::Branch => {
                    // Read branch head snapshot to create branch-specific root_snapshot_lite
                    match SnapshotsIO::read_snapshot_for_vacuum(operator.clone(), &snapshot_ref.loc)
                        .await?
                    {
                        Some(snapshot) => {
                            let branch_root_snapshot_lite = Arc::new(SnapshotLiteExtended {
                                format_version: TableMetaLocationGenerator::snapshot_version(
                                    &snapshot_ref.loc,
                                ),
                                snapshot_id: snapshot.snapshot_id,
                                timestamp: snapshot.timestamp,
                                segments: HashSet::from_iter(snapshot.segments.clone()),
                                table_statistics_location: snapshot.table_statistics_location(),
                            });

                            // Collect segments from all branch snapshots using branch's own root_snapshot_lite
                            Self::collect_snapshots_segments(
                                &snapshots_io,
                                &operator,
                                &snapshot_ref.loc,
                                branch_root_snapshot_lite,
                                max_threads,
                                &mut segments,
                                &status_callback,
                            )
                            .await?;
                        }
                        None => {
                            return Ok(None);
                        }
                    }
                }
            }
        }

        info!(
            "gc orphan: collected segments from {} refs, total segments: {}",
            table_info.meta.refs.len(),
            segments.len()
        );

        Ok(Some(segments))
    }

    /// Update table metadata with updated refs (expired refs removed)
    #[async_backtrace::framed]
    pub async fn update_table_refs_meta(
        &self,
        ctx: &Arc<dyn TableContext>,
        expired_refs: &HashSet<u64>,
    ) -> Result<Vec<String>> {
        let catalog = ctx.get_default_catalog()?;

        let mut retries = 0;
        let mut backoff = set_backoff(None, None, None);
        let mut latest_table_info = self.get_table_info();
        // holding the reference of latest table during retries
        let mut latest_table_ref: Arc<dyn Table>;
        // Step 1: Update table meta if refs changed
        loop {
            let mut new_table_meta = latest_table_info.meta.clone();
            new_table_meta
                .refs
                .retain(|_, val| !expired_refs.contains(&val.id));
            let req = UpdateTableMetaReq {
                table_id: latest_table_info.ident.table_id,
                seq: MatchSeq::Exact(latest_table_info.ident.seq),
                new_table_meta,
                base_snapshot_location: self.snapshot_loc(),
                lvt_check: None,
            };
            match catalog
                .update_single_table_meta(req, latest_table_info)
                .await
            {
                Err(e) if e.code() == ErrorCode::TABLE_VERSION_MISMATCHED => {
                    match backoff.next_backoff() {
                        Some(d) => {
                            tokio::time::sleep(d).await;
                            latest_table_ref = self.refresh(ctx.as_ref()).await?;
                            latest_table_info = latest_table_ref.get_table_info();
                            retries += 1;
                            continue;
                        }
                        None => {
                            return Err(ErrorCode::StorageOther(format!(
                                "update table meta failed after {} retries",
                                retries
                            )));
                        }
                    }
                }
                Err(e) => {
                    return Err(e);
                }
                Ok(_) => {
                    break;
                }
            }
        }

        // Step 2: Cleanup expired ref directories
        let mut dir_to_gc = Vec::with_capacity(expired_refs.len());
        let ref_snapshot_location_prefix = self
            .meta_location_generator()
            .ref_snapshot_location_prefix();
        let op = self.get_operator();
        for ref_id in expired_refs {
            let dir = format!("{}{}/", ref_snapshot_location_prefix, *ref_id);
            op.remove_all(&dir).await.inspect_err(|err| {
                error!("Failed to remove expired ref directory {}: {}", dir, err);
            })?;
            dir_to_gc.push(dir);
        }
        Ok(dir_to_gc)
    }

    pub async fn cleanup_orphan_ref_dirs(&self) -> Result<Vec<String>> {
        let prefix = self
            .meta_location_generator()
            .ref_snapshot_location_prefix();
        let op = self.get_operator();
        let table_info = self.get_table_info();
        let table_seq = table_info.ident.seq;
        let active_ids = table_info
            .meta
            .refs
            .values()
            .map(|snapshot_ref| snapshot_ref.id)
            .collect::<HashSet<_>>();
        let mut removed = Vec::new();
        let mut lister = op.lister(prefix).await?;
        while let Some(entry) = lister.try_next().await? {
            if !entry.metadata().is_dir() {
                continue;
            }

            let dir_path = entry.path();
            let id_str = dir_path.trim_end_matches('/').rsplit('/').next().unwrap();
            let Ok(ref_id) = id_str.parse::<u64>() else {
                continue;
            };
            if table_seq <= ref_id || active_ids.contains(&ref_id) {
                continue;
            }

            match op.remove_all(dir_path).await {
                Ok(_) => {
                    info!(
                        "Removed orphan ref directory '{}' for table {}",
                        dir_path, table_info.desc
                    );
                    removed.push(dir_path.to_string());
                }
                Err(e) => {
                    warn!(
                        "Failed to remove orphan ref directory '{}' for table {}: {}",
                        dir_path, table_info.desc, e
                    );
                }
            }
        }
        Ok(removed)
    }
}

pub async fn vacuum_tables_from_info(
    table_infos: Vec<TableInfo>,
    ctx: Arc<dyn TableContext>,
    vacuum_handler: Arc<VacuumHandlerWrapper>,
) -> Result<()> {
    for table_info in table_infos {
        let table = FuseTable::create_without_refresh_table_info(
            table_info,
            ctx.get_settings().get_s3_storage_class()?,
        )?
        .refresh(ctx.as_ref())
        .await?;
        let fuse_table = FuseTable::try_from_table(table.as_ref())?;
        fuse_table
            .vacuum_table(ctx.clone(), &vacuum_handler, true)
            .await;
    }

    Ok(())
}
