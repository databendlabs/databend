## Copyright 2023 Databend Cloud
##
## Licensed under the Elastic License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     https://www.elastic.co/licensing/elastic-license
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

statement ok
set enable_auto_materialize_cte = 0;

statement ok
DROP DATABASE IF EXISTS test_virtual_db

statement ok
CREATE DATABASE test_virtual_db

statement ok
USE test_virtual_db

statement ok
set enable_experimental_virtual_column = 1;

statement ok
drop table if exists t1

statement ok
create table t1 (a int null, v json null) storage_format = 'native'

statement ok
insert into t1 values(1, parse_json('{"a":[1,2,3],"b":{"c":10},"d":20}'))

query T
explain select a, v['a'][0], v['b']['c'] from t1
----
EvalScalar
├── output columns: [t1.a (#0), v['a'][0] (#2), v['b']['c'] (#3)]
├── expressions: [get_by_keypath(t1.v (#1), '{"a",0}'), get_by_keypath(t1.v (#1), '{"b","c"}')]
├── estimated rows: 1.00
└── TableScan
    ├── table: default.test_virtual_db.t1
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select a, v['a'][0] from t1 where v['b']['c'] = 10
----
EvalScalar
├── output columns: [t1.a (#0), v['a'][0] (#2)]
├── expressions: [get_by_keypath(t1.v (#1), '{"a",0}')]
├── estimated rows: 0.20
└── TableScan
    ├── table: default.test_virtual_db.t1
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [is_true(TRY_CAST(get_by_keypath(t1.v (#1), '{"b","c"}') AS UInt8 NULL) = 10)], limit: NONE]
    └── estimated rows: 0.20

statement ok
drop table if exists t2

statement ok
create table t2 (a int null, v json null) storage_format = 'parquet'

statement ok
insert into t2 values(1, parse_json('{"a":[1,2,3],"b":{"c":10},"d":20}'))

query T
explain select a, v['a'][0], v['b']['c'] from t2
----
TableScan
├── table: default.test_virtual_db.t2
├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#5)]
├── read rows: 1
├── read size: < 1 KiB
├── partitions total: 1
├── partitions scanned: 1
├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
├── push downs: [filters: [], limit: NONE]
├── virtual columns: [v['a'][0], v['b']['c']]
└── estimated rows: 1.00

query T
explain select a, get_by_keypath(v, '{"a",0}') from t2 where get_by_keypath(v, '{"b","c"}') = 10
----
Filter
├── output columns: [t2.a (#0), t2.v['a'][0] (#2)]
├── filters: [is_true(TRY_CAST(t2.v['b']['c'] (#5) AS UInt8 NULL) = 10)]
├── estimated rows: 0.00
└── TableScan
    ├── table: default.test_virtual_db.t2
    ├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#5)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [is_true(TRY_CAST(t2.v['b']['c'] (#5) AS UInt8 NULL) = 10)], limit: NONE]
    ├── virtual columns: [v['a'][0], v['b']['c']]
    └── estimated rows: 1.00

query T
explain select a, get_by_keypath_string(v, '{"a",0}') from t2 where get_by_keypath_string(v, '{"b","c"}') like '%10%'
----
EvalScalar
├── output columns: [t2.a (#0), get_by_keypath_string(v, '{"a",0}') (#7)]
├── expressions: [CAST(t2.v['a'][0] (#2) AS String NULL)]
├── estimated rows: 0.50
└── Filter
    ├── output columns: [t2.a (#0), t2.v['a'][0] (#2)]
    ├── filters: [is_true(like(CAST(t2.v['b']['c'] (#5) AS String NULL), '%10%'))]
    ├── estimated rows: 0.50
    └── TableScan
        ├── table: default.test_virtual_db.t2
        ├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#5)]
        ├── read rows: 1
        ├── read size: < 1 KiB
        ├── partitions total: 1
        ├── partitions scanned: 1
        ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
        ├── push downs: [filters: [is_true(like(CAST(t2.v['b']['c'] (#5) AS String NULL), '%10%'))], limit: NONE]
        ├── virtual columns: [v['a'][0], v['b']['c']]
        └── estimated rows: 1.00

query T
explain select a, get(v, 'd') from t2 where get(v, 'd') = 20
----
Filter
├── output columns: [t2.a (#0), t2.v['d'] (#6)]
├── filters: [is_true(TRY_CAST(t2.v['d'] (#6) AS UInt8 NULL) = 20)]
├── estimated rows: 0.00
└── TableScan
    ├── table: default.test_virtual_db.t2
    ├── output columns: [a (#0), v['d'] (#6)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [is_true(TRY_CAST(t2.v['d'] (#6) AS UInt8 NULL) = 20)], limit: NONE]
    ├── virtual columns: [v['d']]
    └── estimated rows: 1.00

query T
explain select a, get(get(v, 'a'), 0), get(get(v, 'b'), 'c') from t2;
----
TableScan
├── table: default.test_virtual_db.t2
├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#5)]
├── read rows: 1
├── read size: < 1 KiB
├── partitions total: 1
├── partitions scanned: 1
├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
├── push downs: [filters: [], limit: NONE]
├── virtual columns: [v['a'][0], v['b']['c']]
└── estimated rows: 1.00

query T
explain select v['d']::string, v['d']::int from t2;
----
EvalScalar
├── output columns: [v['d'] (#7), v['d'] (#8)]
├── expressions: [CAST(t2.v['d'] (#6) AS String NULL), CAST(t2.v['d'] (#6) AS Int32 NULL)]
├── estimated rows: 1.00
└── TableScan
    ├── table: default.test_virtual_db.t2
    ├── output columns: [v['d'] (#6)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [], limit: NONE]
    ├── virtual columns: [v['d']]
    └── estimated rows: 1.00

query T
select v['d']::string, v['d']::int from t2;
----
20 20

query T
explain select a, get_string(v, 'd') from t2 where get_string(v, 'd') like '%20%'
----
EvalScalar
├── output columns: [t2.a (#0), get_string(v, 'd') (#7)]
├── expressions: [CAST(t2.v['d'] (#6) AS String NULL)]
├── estimated rows: 0.50
└── Filter
    ├── output columns: [t2.a (#0), t2.v['d'] (#6)]
    ├── filters: [is_true(like(CAST(t2.v['d'] (#6) AS String NULL), '%20%'))]
    ├── estimated rows: 0.50
    └── TableScan
        ├── table: default.test_virtual_db.t2
        ├── output columns: [a (#0), v['d'] (#6)]
        ├── read rows: 1
        ├── read size: < 1 KiB
        ├── partitions total: 1
        ├── partitions scanned: 1
        ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
        ├── push downs: [filters: [is_true(like(CAST(t2.v['d'] (#6) AS String NULL), '%20%'))], limit: NONE]
        ├── virtual columns: [v['d']]
        └── estimated rows: 1.00

query T
explain select t2.a, t2.v['b'] from t2 left outer join t1 on t2.v['b']['c'] = t1.a
----
EvalScalar
├── output columns: [t2.a (#0), v['b'] (#9)]
├── expressions: [get_by_keypath(t2.v (#1), '{"b"}')]
├── estimated rows: 1.00
└── HashJoin
    ├── output columns: [t2.a (#0), t2.v (#1)]
    ├── join type: LEFT OUTER
    ├── build keys: [t1.a (#7)]
    ├── probe keys: [CAST(t2.v['b']['c'] (#5) AS Int32 NULL)]
    ├── keys is null equal: [false]
    ├── filters: []
    ├── build join filters:
    ├── estimated rows: 1.00
    ├── TableScan(Build)
    │   ├── table: default.test_virtual_db.t1
    │   ├── output columns: [a (#7)]
    │   ├── read rows: 1
    │   ├── read size: < 1 KiB
    │   ├── partitions total: 1
    │   ├── partitions scanned: 1
    │   ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    │   ├── push downs: [filters: [], limit: NONE]
    │   └── estimated rows: 1.00
    └── TableScan(Probe)
        ├── table: default.test_virtual_db.t2
        ├── output columns: [a (#0), v (#1), v['b']['c'] (#5)]
        ├── read rows: 1
        ├── read size: < 1 KiB
        ├── partitions total: 1
        ├── partitions scanned: 1
        ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
        ├── push downs: [filters: [], limit: NONE]
        ├── virtual columns: [v['b']['c']]
        └── estimated rows: 1.00

query T
explain select t2.a, t2.v['b']['c'] from t2 left outer join t1 on t2.v['b']['c'] = t1.a where t2.v['a'][0] = 1;
----
HashJoin
├── output columns: [t2.a (#0), t2.v['b']['c'] (#5)]
├── join type: RIGHT OUTER
├── build keys: [CAST(t2.v['b']['c'] (#5) AS Int32 NULL)]
├── probe keys: [t1.a (#7)]
├── keys is null equal: [false]
├── filters: []
├── build join filters:
│   └── filter id:0, build key:CAST(t2.v['b']['c'] (#5) AS Int32 NULL), probe key:t1.a (#7), filter type:inlist,min_max
├── estimated rows: 0.00
├── Filter(Build)
│   ├── output columns: [t2.a (#0), t2.v['b']['c'] (#5)]
│   ├── filters: [is_true(TRY_CAST(t2.v['a'][0] (#2) AS UInt8 NULL) = 1)]
│   ├── estimated rows: 0.00
│   └── TableScan
│       ├── table: default.test_virtual_db.t2
│       ├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#5)]
│       ├── read rows: 1
│       ├── read size: < 1 KiB
│       ├── partitions total: 1
│       ├── partitions scanned: 1
│       ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
│       ├── push downs: [filters: [is_true(TRY_CAST(t2.v['a'][0] (#2) AS UInt8 NULL) = 1)], limit: NONE]
│       ├── virtual columns: [v['a'][0], v['b']['c']]
│       └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t1
    ├── output columns: [a (#7)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [], limit: NONE]
    ├── apply join filters: [#0]
    └── estimated rows: 1.00

query T
explain select * from t1 join t2 on t2.v['b']['c'] = t2.a;
----
HashJoin
├── output columns: [t1.a (#0), t1.v (#1), t2.a (#2), t2.v (#3)]
├── join type: CROSS
├── build keys: []
├── probe keys: []
├── keys is null equal: []
├── filters: []
├── build join filters:
├── estimated rows: 0.20
├── Filter(Build)
│   ├── output columns: [t2.a (#2), t2.v (#3)]
│   ├── filters: [is_true(TRY_CAST(t2.v['b']['c'] (#7) AS Int32 NULL) = t2.a (#2))]
│   ├── estimated rows: 0.20
│   └── TableScan
│       ├── table: default.test_virtual_db.t2
│       ├── output columns: [a (#2), v (#3), v['b']['c'] (#7)]
│       ├── read rows: 1
│       ├── read size: < 1 KiB
│       ├── partitions total: 1
│       ├── partitions scanned: 1
│       ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
│       ├── push downs: [filters: [is_true(TRY_CAST(t2.v['b']['c'] (#7) AS Int32 NULL) = t2.a (#2))], limit: NONE]
│       ├── virtual columns: [v['b']['c']]
│       └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t1
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select * from t1 join t2 on t2.v['b']['c'] > t2.a;
----
HashJoin
├── output columns: [t1.a (#0), t1.v (#1), t2.a (#2), t2.v (#3)]
├── join type: CROSS
├── build keys: []
├── probe keys: []
├── keys is null equal: []
├── filters: []
├── build join filters:
├── estimated rows: 0.20
├── Filter(Build)
│   ├── output columns: [t2.a (#2), t2.v (#3)]
│   ├── filters: [is_true(TRY_CAST(t2.v['b']['c'] (#7) AS Int32 NULL) > t2.a (#2))]
│   ├── estimated rows: 0.20
│   └── TableScan
│       ├── table: default.test_virtual_db.t2
│       ├── output columns: [a (#2), v (#3), v['b']['c'] (#7)]
│       ├── read rows: 1
│       ├── read size: < 1 KiB
│       ├── partitions total: 1
│       ├── partitions scanned: 1
│       ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
│       ├── push downs: [filters: [is_true(TRY_CAST(t2.v['b']['c'] (#7) AS Int32 NULL) > t2.a (#2))], limit: NONE]
│       ├── virtual columns: [v['b']['c']]
│       └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t1
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select t2.v['b']['c'], sum(t1.a) from t2 join t1 on t2.v['b']['c'] = t1.a group by t2.v['b']['c'];
----
AggregateFinal
├── output columns: [sum(t1.a) (#9), t2.v['b']['c'] (#5)]
├── group by: [v['b']['c']]
├── aggregate functions: [sum(a)]
├── estimated rows: 1.00
└── AggregatePartial
    ├── group by: [v['b']['c']]
    ├── aggregate functions: [sum(a)]
    ├── estimated rows: 1.00
    └── HashJoin
        ├── output columns: [t2.v['b']['c'] (#5), t1.a (#7)]
        ├── join type: INNER
        ├── build keys: [t1.a (#7)]
        ├── probe keys: [CAST(t2.v['b']['c'] (#5) AS Int32 NULL)]
        ├── keys is null equal: [false]
        ├── filters: []
        ├── build join filters:
        ├── estimated rows: 1.00
        ├── TableScan(Build)
        │   ├── table: default.test_virtual_db.t1
        │   ├── output columns: [a (#7)]
        │   ├── read rows: 1
        │   ├── read size: < 1 KiB
        │   ├── partitions total: 1
        │   ├── partitions scanned: 1
        │   ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
        │   ├── push downs: [filters: [], limit: NONE]
        │   └── estimated rows: 1.00
        └── TableScan(Probe)
            ├── table: default.test_virtual_db.t2
            ├── output columns: [v['b']['c'] (#5)]
            ├── read rows: 1
            ├── read size: < 1 KiB
            ├── partitions total: 1
            ├── partitions scanned: 1
            ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
            ├── push downs: [filters: [], limit: NONE]
            ├── virtual columns: [v['b']['c']]
            └── estimated rows: 1.00


statement ok
CREATE OR REPLACE TABLE data_source_a (
    entity_id VARCHAR,
    source_id VARCHAR,
    metadata_object VARIANT,
    content_object VARIANT,
    refresh_time TIMESTAMP
);

statement ok
CREATE OR REPLACE TABLE config_table (
    entity_id VARCHAR,
    source_id VARCHAR,
    process_mode VARCHAR
);

statement ok
INSERT INTO data_source_a VALUES
('ENTITY1', 'SRC1', '{"type": "T1"}', '{"event_date": 1609459200000, "category_a": "CA1", "category_b": "CB1"}', CURRENT_TIMESTAMP());

statement ok
INSERT INTO config_table VALUES('ENTITY1', 'SRC1', 'standard_mode');

query T
EXPLAIN WITH processed_dates AS (
    SELECT
        a.entity_id,
        a.source_id,
        TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE AS event_date
    FROM
        data_source_a a
    JOIN
        config_table c
    ON
        c.entity_id = a.entity_id
        AND c.source_id = a.source_id
        AND c.process_mode = 'standard_mode'
    GROUP BY
        1, 2, 3
),
data_aggregation AS (
    SELECT
        a.entity_id,
        a.source_id,
        COALESCE(a.metadata_object:type::VARCHAR, 'Unknown') AS type_code,
        a.content_object:category_a::VARCHAR AS primary_category,
        a.content_object:category_b::VARCHAR AS secondary_category,
        p.event_date AS event_date
    FROM
        data_source_a a
    JOIN
        processed_dates p
    ON
        p.entity_id = a.entity_id
        AND p.source_id = a.source_id
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE = p.event_date
    WHERE
        a.content_object:category_a::VARCHAR IS NOT NULL
        AND a.content_object:category_b::VARCHAR IS NOT NULL
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE IS NOT NULL
    GROUP BY
        1, 2, 3, 4, 5, 6
)
SELECT * FROM data_aggregation;
----
EvalScalar
├── output columns: [a.entity_id (#0), a.source_id (#1), event_date (#22), type_code (#26), primary_category (#27), secondary_category (#28)]
├── expressions: [group_item (#23), group_item (#24), group_item (#25)]
├── estimated rows: 0.04
└── AggregateFinal
    ├── output columns: [a.entity_id (#0), a.source_id (#1), type_code (#23), primary_category (#24), secondary_category (#25), event_date (#22)]
    ├── group by: [entity_id, source_id, type_code, primary_category, secondary_category, event_date]
    ├── aggregate functions: []
    ├── estimated rows: 0.04
    └── AggregatePartial
        ├── group by: [entity_id, source_id, type_code, primary_category, secondary_category, event_date]
        ├── aggregate functions: []
        ├── estimated rows: 0.04
        └── EvalScalar
            ├── output columns: [a.entity_id (#0), a.source_id (#1), event_date (#22), type_code (#23), primary_category (#24), secondary_category (#25)]
            ├── expressions: [if(CAST(is_not_null(CAST(a.metadata_object['type'] (#5) AS String NULL)) AS Boolean NULL), CAST(assume_not_null(CAST(a.metadata_object['type'] (#5) AS String NULL)) AS String NULL), true, 'Unknown', NULL), CAST(a.content_object['category_a'] (#6) AS String NULL), CAST(a.content_object['category_b'] (#7) AS String NULL)]
            ├── estimated rows: 0.04
            └── HashJoin
                ├── output columns: [a.entity_id (#0), a.source_id (#1), a.metadata_object['type'] (#5), a.content_object['category_a'] (#6), a.content_object['category_b'] (#7), event_date (#22)]
                ├── join type: INNER
                ├── build keys: [p.entity_id (#9), p.source_id (#10), p.event_date (#22)]
                ├── probe keys: [a.entity_id (#0), a.source_id (#1), CAST(CAST(CAST(a.content_object['event_date'] (#8) AS Int64 NULL) AS Timestamp NULL) AS Date NULL)]
                ├── keys is null equal: [false, false, false]
                ├── filters: []
                ├── build join filters:
                │   ├── filter id:2, build key:p.entity_id (#9), probe key:a.entity_id (#0), filter type:inlist,min_max
                │   └── filter id:3, build key:p.source_id (#10), probe key:a.source_id (#1), filter type:inlist,min_max
                ├── estimated rows: 0.04
                ├── EvalScalar(Build)
                │   ├── output columns: [a.entity_id (#9), a.source_id (#10), event_date (#22)]
                │   ├── expressions: [group_item (#21)]
                │   ├── estimated rows: 0.20
                │   └── AggregateFinal
                │       ├── output columns: [a.entity_id (#9), a.source_id (#10), event_date (#21)]
                │       ├── group by: [entity_id, source_id, event_date]
                │       ├── aggregate functions: []
                │       ├── estimated rows: 0.20
                │       └── AggregatePartial
                │           ├── group by: [entity_id, source_id, event_date]
                │           ├── aggregate functions: []
                │           ├── estimated rows: 0.20
                │           └── EvalScalar
                │               ├── output columns: [a.entity_id (#9), a.source_id (#10), event_date (#21)]
                │               ├── expressions: [CAST(CAST(CAST(a.content_object['event_date'] (#17) AS Int64 NULL) AS Timestamp NULL) AS Date NULL)]
                │               ├── estimated rows: 0.20
                │               └── HashJoin
                │                   ├── output columns: [a.content_object['event_date'] (#17), a.entity_id (#9), a.source_id (#10)]
                │                   ├── join type: INNER
                │                   ├── build keys: [a.entity_id (#9), a.source_id (#10)]
                │                   ├── probe keys: [c.entity_id (#18), c.source_id (#19)]
                │                   ├── keys is null equal: [false, false]
                │                   ├── filters: []
                │                   ├── build join filters:
                │                   │   ├── filter id:0, build key:a.entity_id (#9), probe key:c.entity_id (#18), filter type:inlist,min_max
                │                   │   └── filter id:1, build key:a.source_id (#10), probe key:c.source_id (#19), filter type:inlist,min_max
                │                   ├── estimated rows: 0.20
                │                   ├── Filter(Build)
                │                   │   ├── output columns: [a.entity_id (#9), a.source_id (#10), a.content_object['event_date'] (#17)]
                │                   │   ├── filters: [is_not_null(CAST(CAST(CAST(a.content_object['event_date'] (#17) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))]
                │                   │   ├── estimated rows: 0.20
                │                   │   └── TableScan
                │                   │       ├── table: default.test_virtual_db.data_source_a
                │                   │       ├── output columns: [entity_id (#9), source_id (#10), content_object['event_date'] (#17)]
                │                   │       ├── read rows: 1
                │                   │       ├── read size: < 1 KiB
                │                   │       ├── partitions total: 1
                │                   │       ├── partitions scanned: 1
                │                   │       ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
                │                   │       ├── push downs: [filters: [is_not_null(CAST(CAST(CAST(a.content_object['event_date'] (#17) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))], limit: NONE]
                │                   │       ├── virtual columns: [content_object['event_date']]
                │                   │       └── estimated rows: 1.00
                │                   └── Filter(Probe)
                │                       ├── output columns: [c.entity_id (#18), c.source_id (#19)]
                │                       ├── filters: [is_true(c.process_mode (#20) = 'standard_mode')]
                │                       ├── estimated rows: 1.00
                │                       └── TableScan
                │                           ├── table: default.test_virtual_db.config_table
                │                           ├── output columns: [entity_id (#18), source_id (#19), process_mode (#20)]
                │                           ├── read rows: 1
                │                           ├── read size: < 1 KiB
                │                           ├── partitions total: 1
                │                           ├── partitions scanned: 1
                │                           ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1, bloom pruning: 1 to 1>]
                │                           ├── push downs: [filters: [is_true(config_table.process_mode (#20) = 'standard_mode')], limit: NONE]
                │                           ├── apply join filters: [#0, #1]
                │                           └── estimated rows: 1.00
                └── Filter(Probe)
                    ├── output columns: [a.entity_id (#0), a.source_id (#1), a.metadata_object['type'] (#5), a.content_object['category_a'] (#6), a.content_object['category_b'] (#7), a.content_object['event_date'] (#8)]
                    ├── filters: [is_not_null(CAST(a.content_object['category_a'] (#6) AS String NULL)), is_not_null(CAST(a.content_object['category_b'] (#7) AS String NULL)), is_not_null(CAST(CAST(CAST(a.content_object['event_date'] (#8) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))]
                    ├── estimated rows: 0.20
                    └── TableScan
                        ├── table: default.test_virtual_db.data_source_a
                        ├── output columns: [entity_id (#0), source_id (#1), metadata_object['type'] (#5), content_object['category_a'] (#6), content_object['category_b'] (#7), content_object['event_date'] (#8)]
                        ├── read rows: 1
                        ├── read size: < 1 KiB
                        ├── partitions total: 1
                        ├── partitions scanned: 1
                        ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
                        ├── push downs: [filters: [and_filters(and_filters(is_not_null(CAST(a.content_object['category_a'] (#6) AS String NULL)), is_not_null(CAST(a.content_object['category_b'] (#7) AS String NULL))), is_not_null(CAST(CAST(CAST(a.content_object['event_date'] (#8) AS Int64 NULL) AS Timestamp NULL) AS Date NULL)))], limit: NONE]
                        ├── apply join filters: [#2, #3]
                        ├── virtual columns: [content_object['category_a'], content_object['category_b'], content_object['event_date'], metadata_object['type']]
                        └── estimated rows: 1.00

query TTTTTT
WITH processed_dates AS (
    SELECT
        a.entity_id,
        a.source_id,
        TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE AS event_date
    FROM
        data_source_a a
    JOIN
        config_table c
    ON
        c.entity_id = a.entity_id
        AND c.source_id = a.source_id
        AND c.process_mode = 'standard_mode'
    GROUP BY
        1, 2, 3
),
data_aggregation AS (
    SELECT
        a.entity_id,
        a.source_id,
        COALESCE(a.metadata_object:type::VARCHAR, 'Unknown') AS type_code,
        a.content_object:category_a::VARCHAR AS primary_category,
        a.content_object:category_b::VARCHAR AS secondary_category,
        p.event_date AS event_date
    FROM
        data_source_a a
    JOIN
        processed_dates p
    ON
        p.entity_id = a.entity_id
        AND p.source_id = a.source_id
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE = p.event_date
    WHERE
        a.content_object:category_a::VARCHAR IS NOT NULL
        AND a.content_object:category_b::VARCHAR IS NOT NULL
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE IS NOT NULL
    GROUP BY
        1, 2, 3, 4, 5, 6
)
SELECT * FROM data_aggregation;
----
ENTITY1 SRC1 T1 CA1 CB1 2021-01-01

statement ok
CREATE OR REPLACE TABLE data_main (
    record_id VARCHAR,
    category_id VARCHAR,
    data_object VARIANT
);

statement ok
CREATE OR REPLACE TABLE data_staging (
    data_object VARIANT
);

statement ok
INSERT INTO data_main (record_id, category_id, data_object) VALUES
('rec1', 'cat1', '{"timestamp": 1625000000000, "metadata": {"category_id": "cat1", "timestamp": 1625000000000}}');

statement ok
INSERT INTO data_staging (data_object) VALUES
('{"unique_key": "rec1", "metadata": {"category_id": "cat1"}, "timestamp": 1625100000000}');

query T
EXPLAIN MERGE INTO data_main target
USING (
  SELECT
    data_object:unique_key AS record_id,
    data_object:metadata.category_id AS category_id,
    1624900000000 AS reference_time
  FROM data_staging
) source
ON target.record_id = source.record_id AND target.category_id = source.category_id
WHEN MATCHED THEN
UPDATE SET
  data_object = json_object_insert(
    target.data_object,
    'metadata',
    json_object_insert(
      json_object_insert(
        target.data_object:metadata,
        'reference_time',
        source.reference_time::variant,
        true
      ),
      'time_difference',
      coalesce((target.data_object:metadata.timestamp - source.reference_time) / 24 / 60 / 60 / 1000::variant, null::variant),
      true
    ),
    true
  );
----
CommitSink
└── DataMutation
    ├── target table: [catalog: default] [database: test_virtual_db] [table: data_main]
    ├── matched update: [condition: None, update set data_object = if(CAST(_predicate (#18446744073709551615) AS Boolean NULL), object_insert(target.data_object (#7), 'metadata', object_insert(object_insert(get_by_keypath(target.data_object (#7), '{"metadata"}'), 'reference_time', CAST(source.reference_time (#4) AS Variant), true), 'time_difference', if(CAST(is_not_null((TRY_CAST(get_by_keypath(target.data_object (#7), '{"metadata","timestamp"}') AS UInt8 NULL) - CAST(source.reference_time (#4) AS UInt64 NULL)) / 24 / 60 / 60 / NULL) AS Boolean NULL), CAST(assume_not_null((TRY_CAST(get_by_keypath(target.data_object (#7), '{"metadata","timestamp"}') AS UInt8 NULL) - CAST(source.reference_time (#4) AS UInt64 NULL)) / 24 / 60 / 60 / NULL) AS Float64 NULL), NULL), true), true), target.data_object (#7))]
    └── RowFetch
        ├── output columns: [target.record_id (#5), target.category_id (#6), target._row_id (#8), data_staging.data_object['metadata']['category_id'] (#1), data_staging.data_object['unique_key'] (#3), reference_time (#4), target.data_object (#7)]
        ├── columns to fetch: [data_object]
        └── HashJoin
            ├── output columns: [target.record_id (#5), target.category_id (#6), target._row_id (#8), data_staging.data_object['metadata']['category_id'] (#1), data_staging.data_object['unique_key'] (#3), reference_time (#4)]
            ├── join type: INNER
            ├── build keys: [CAST(source.record_id (#3) AS String NULL), CAST(source.category_id (#1) AS String NULL)]
            ├── probe keys: [target.record_id (#5), target.category_id (#6)]
            ├── keys is null equal: [false, false]
            ├── filters: []
            ├── build join filters:
            │   ├── filter id:0, build key:CAST(source.record_id (#3) AS String NULL), probe key:target.record_id (#5), filter type:inlist,min_max
            │   └── filter id:1, build key:CAST(source.category_id (#1) AS String NULL), probe key:target.category_id (#6), filter type:inlist,min_max
            ├── estimated rows: 1.00
            ├── EvalScalar(Build)
            │   ├── output columns: [data_staging.data_object['metadata']['category_id'] (#1), data_staging.data_object['unique_key'] (#3), reference_time (#4)]
            │   ├── expressions: [1624900000000]
            │   ├── estimated rows: 1.00
            │   └── TableScan
            │       ├── table: default.test_virtual_db.data_staging
            │       ├── output columns: [data_object['metadata']['category_id'] (#1), data_object['unique_key'] (#3)]
            │       ├── read rows: 1
            │       ├── read size: < 1 KiB
            │       ├── partitions total: 1
            │       ├── partitions scanned: 1
            │       ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
            │       ├── push downs: [filters: [], limit: NONE]
            │       ├── virtual columns: [data_object['metadata']['category_id'], data_object['unique_key']]
            │       └── estimated rows: 1.00
            └── TableScan(Probe)
                ├── table: default.test_virtual_db.data_main
                ├── output columns: [record_id (#5), category_id (#6), _row_id (#8)]
                ├── read rows: 1
                ├── read size: < 1 KiB
                ├── partitions total: 1
                ├── partitions scanned: 1
                ├── pruning stats: [segments: <range pruning: 1 to 1>, blocks: <range pruning: 1 to 1>]
                ├── push downs: [filters: [], limit: NONE]
                ├── apply join filters: [#0, #1]
                └── estimated rows: 1.00

query TTT
SELECT * FROM data_main;
----
rec1 cat1 {"metadata":{"category_id":"cat1","timestamp":1625000000000},"timestamp":1625000000000}

statement ok
drop table t1

statement ok
drop table t2

statement ok
drop table data_source_a

statement ok
drop table config_table

statement ok
drop table data_main

statement ok
drop table data_staging

statement ok
set enable_experimental_virtual_column = 0;

statement ok
USE default

statement ok
DROP DATABASE IF EXISTS test_virtual_db
