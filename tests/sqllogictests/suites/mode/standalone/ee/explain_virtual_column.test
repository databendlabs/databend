## Copyright 2023 Databend Cloud
##
## Licensed under the Elastic License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     https://www.elastic.co/licensing/elastic-license
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

statement ok
set enable_auto_materialize_cte = 0;

statement ok
DROP DATABASE IF EXISTS test_virtual_db

statement ok
CREATE DATABASE test_virtual_db

statement ok
USE test_virtual_db

statement ok
set enable_experimental_virtual_column = 1;

statement ok
drop table if exists t1

statement ok
create table t1 (a int null, v json null) storage_format = 'native'

statement ok
insert into t1 values(1, parse_json('{"a":[1,2,3],"b":{"c":10},"d":20}'))

query T
explain select a, v['a'][0], v['b']['c'] from t1
----
EvalScalar
├── output columns: [t1.a (#0), v['a'][0] (#2), v['b']['c'] (#3)]
├── expressions: [get_by_keypath(t1.v (#1), '{"a",0}'), get_by_keypath(t1.v (#1), '{"b","c"}')]
├── estimated rows: 1.00
└── TableScan
    ├── table: default.test_virtual_db.t1
    ├── scan id: 0
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select a, v['a'][0] from t1 where v['b']['c'] = 10
----
EvalScalar
├── output columns: [t1.a (#0), v['a'][0] (#2)]
├── expressions: [get_by_keypath(t1.v (#1), '{"a",0}')]
├── estimated rows: 0.20
└── TableScan
    ├── table: default.test_virtual_db.t1
    ├── scan id: 0
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [is_true(TRY_CAST(get_by_keypath(t1.v (#1), '{"b","c"}') AS UInt8 NULL) = 10)], limit: NONE]
    └── estimated rows: 0.20

statement ok
drop table if exists t2

statement ok
create table t2 (a int null, v json null) storage_format = 'parquet'

statement ok
insert into t2 values(1, parse_json('{"a":[1,2,3],"b":{"c":10},"d":20}'))

query T
explain select a, v['a'][0], v['b']['c'] from t2
----
TableScan
├── table: default.test_virtual_db.t2
├── scan id: 0
├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#3)]
├── read rows: 1
├── read size: < 1 KiB
├── partitions total: 1
├── partitions scanned: 1
├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
├── push downs: [filters: [], limit: NONE]
├── virtual columns: [v['a'][0], v['b']['c']]
└── estimated rows: 1.00

query T
explain select a, get_by_keypath(v, '{"a",0}') from t2 where get_by_keypath(v, '{"b","c"}') = 10
----
Filter
├── output columns: [t2.a (#0), t2.v['a'][0] (#2)]
├── filters: [is_true(TRY_CAST(t2.v['b']['c'] (#3) AS UInt8 NULL) = 10)]
├── estimated rows: 0.20
└── TableScan
    ├── table: default.test_virtual_db.t2
    ├── scan id: 0
    ├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#3)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [is_true(TRY_CAST(t2.v['b']['c'] (#3) AS UInt8 NULL) = 10)], limit: NONE]
    ├── virtual columns: [v['a'][0], v['b']['c']]
    └── estimated rows: 1.00

query T
explain select a, get_by_keypath_string(v, '{"a",0}') from t2 where get_by_keypath_string(v, '{"b","c"}') like '%10%'
----
EvalScalar
├── output columns: [t2.a (#0), get_by_keypath_string(v, '{"a",0}') (#4)]
├── expressions: [CAST(t2.v['a'][0] (#2) AS String NULL)]
├── estimated rows: 0.50
└── Filter
    ├── output columns: [t2.a (#0), t2.v['a'][0] (#2)]
    ├── filters: [is_true(like(CAST(t2.v['b']['c'] (#3) AS String NULL), '%10%'))]
    ├── estimated rows: 0.50
    └── TableScan
        ├── table: default.test_virtual_db.t2
        ├── scan id: 0
        ├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#3)]
        ├── read rows: 1
        ├── read size: < 1 KiB
        ├── partitions total: 1
        ├── partitions scanned: 1
        ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
        ├── push downs: [filters: [is_true(like(CAST(t2.v['b']['c'] (#3) AS String NULL), '%10%'))], limit: NONE]
        ├── virtual columns: [v['a'][0], v['b']['c']]
        └── estimated rows: 1.00

query T
explain select a, get(v, 'd') from t2 where get(v, 'd') = 20
----
Filter
├── output columns: [t2.a (#0), t2.v['d'] (#2)]
├── filters: [is_true(TRY_CAST(t2.v['d'] (#2) AS UInt8 NULL) = 20)]
├── estimated rows: 0.20
└── TableScan
    ├── table: default.test_virtual_db.t2
    ├── scan id: 0
    ├── output columns: [a (#0), v['d'] (#2)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [is_true(TRY_CAST(t2.v['d'] (#2) AS UInt8 NULL) = 20)], limit: NONE]
    ├── virtual columns: [v['d']]
    └── estimated rows: 1.00

query T
explain select a, get(get(v, 'a'), 0), get(get(v, 'b'), 'c') from t2;
----
TableScan
├── table: default.test_virtual_db.t2
├── scan id: 0
├── output columns: [a (#0), v['a'][0] (#2), v['b']['c'] (#3)]
├── read rows: 1
├── read size: < 1 KiB
├── partitions total: 1
├── partitions scanned: 1
├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
├── push downs: [filters: [], limit: NONE]
├── virtual columns: [v['a'][0], v['b']['c']]
└── estimated rows: 1.00

query T
explain select v['d']::string, v['d']::int from t2;
----
EvalScalar
├── output columns: [v['d'] (#3), v['d'] (#4)]
├── expressions: [CAST(t2.v['d'] (#2) AS String NULL), CAST(t2.v['d'] (#2) AS Int32 NULL)]
├── estimated rows: 1.00
└── TableScan
    ├── table: default.test_virtual_db.t2
    ├── scan id: 0
    ├── output columns: [v['d'] (#2)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [], limit: NONE]
    ├── virtual columns: [v['d']]
    └── estimated rows: 1.00

query TI
select v['d']::string, v['d']::int from t2;
----
20 20

query T
explain select a, get_string(v, 'd') from t2 where get_string(v, 'd') like '%20%'
----
EvalScalar
├── output columns: [t2.a (#0), get_string(v, 'd') (#3)]
├── expressions: [CAST(t2.v['d'] (#2) AS String NULL)]
├── estimated rows: 0.50
└── Filter
    ├── output columns: [t2.a (#0), t2.v['d'] (#2)]
    ├── filters: [is_true(like(CAST(t2.v['d'] (#2) AS String NULL), '%20%'))]
    ├── estimated rows: 0.50
    └── TableScan
        ├── table: default.test_virtual_db.t2
        ├── scan id: 0
        ├── output columns: [a (#0), v['d'] (#2)]
        ├── read rows: 1
        ├── read size: < 1 KiB
        ├── partitions total: 1
        ├── partitions scanned: 1
        ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
        ├── push downs: [filters: [is_true(like(CAST(t2.v['d'] (#2) AS String NULL), '%20%'))], limit: NONE]
        ├── virtual columns: [v['d']]
        └── estimated rows: 1.00

query T
explain select t2.a, t2.v['b'] from t2 left outer join t1 on t2.v['b']['c'] = t1.a
----
HashJoin
├── output columns: [t2.a (#0), t2.v['b'] (#6)]
├── join type: LEFT OUTER
├── build keys: []
├── probe keys: []
├── keys is null equal: []
├── filters: [TRY_CAST(t2.v['b']['c'] (#4) AS Int32 NULL) = t1.a (#2)]
├── estimated rows: 1.00
├── TableScan(Build)
│   ├── table: default.test_virtual_db.t1
│   ├── scan id: 1
│   ├── output columns: [a (#2)]
│   ├── read rows: 1
│   ├── read size: < 1 KiB
│   ├── partitions total: 1
│   ├── partitions scanned: 1
│   ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
│   ├── push downs: [filters: [], limit: NONE]
│   └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t2
    ├── scan id: 0
    ├── output columns: [a (#0), v['b']['c'] (#4), v['b'] (#6)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [], limit: NONE]
    ├── virtual columns: [v['b'], v['b']['c']]
    └── estimated rows: 1.00

query T
explain select t2.a, t2.v['b']['c'] from t2 left outer join t1 on t2.v['b']['c'] = t1.a where t2.v['a'][0] = 1;
----
HashJoin
├── output columns: [t2.a (#0), t2.v['b']['c'] (#4)]
├── join type: RIGHT OUTER
├── build keys: []
├── probe keys: []
├── keys is null equal: []
├── filters: [TRY_CAST(t2.v['b']['c'] (#4) AS Int32 NULL) = t1.a (#2)]
├── estimated rows: 0.20
├── Filter(Build)
│   ├── output columns: [t2.a (#0), t2.v['b']['c'] (#4)]
│   ├── filters: [is_true(TRY_CAST(t2.v['a'][0] (#6) AS UInt8 NULL) = 1)]
│   ├── estimated rows: 0.20
│   └── TableScan
│       ├── table: default.test_virtual_db.t2
│       ├── scan id: 0
│       ├── output columns: [a (#0), v['b']['c'] (#4), v['a'][0] (#6)]
│       ├── read rows: 1
│       ├── read size: < 1 KiB
│       ├── partitions total: 1
│       ├── partitions scanned: 1
│       ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
│       ├── push downs: [filters: [is_true(TRY_CAST(t2.v['a'][0] (#6) AS UInt8 NULL) = 1)], limit: NONE]
│       ├── virtual columns: [v['a'][0], v['b']['c']]
│       └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t1
    ├── scan id: 1
    ├── output columns: [a (#2)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select * from t1 join t2 on t2.v['b']['c'] = t2.a;
----
HashJoin
├── output columns: [t1.a (#0), t1.v (#1), t2.a (#2), t2.v (#3)]
├── join type: CROSS
├── build keys: []
├── probe keys: []
├── keys is null equal: []
├── filters: []
├── estimated rows: 0.20
├── Filter(Build)
│   ├── output columns: [t2.a (#2), t2.v (#3)]
│   ├── filters: [is_true(TRY_CAST(t2.v['b']['c'] (#4) AS Int32 NULL) = t2.a (#2))]
│   ├── estimated rows: 0.20
│   └── TableScan
│       ├── table: default.test_virtual_db.t2
│       ├── scan id: 1
│       ├── output columns: [a (#2), v (#3), v['b']['c'] (#4)]
│       ├── read rows: 1
│       ├── read size: < 1 KiB
│       ├── partitions total: 1
│       ├── partitions scanned: 1
│       ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
│       ├── push downs: [filters: [is_true(TRY_CAST(t2.v['b']['c'] (#4) AS Int32 NULL) = t2.a (#2))], limit: NONE]
│       ├── virtual columns: [v['b']['c']]
│       └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t1
    ├── scan id: 0
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select * from t1 join t2 on t2.v['b']['c'] > t2.a;
----
HashJoin
├── output columns: [t1.a (#0), t1.v (#1), t2.a (#2), t2.v (#3)]
├── join type: CROSS
├── build keys: []
├── probe keys: []
├── keys is null equal: []
├── filters: []
├── estimated rows: 0.20
├── Filter(Build)
│   ├── output columns: [t2.a (#2), t2.v (#3)]
│   ├── filters: [is_true(TRY_CAST(t2.v['b']['c'] (#5) AS Int32 NULL) > t2.a (#2))]
│   ├── estimated rows: 0.20
│   └── TableScan
│       ├── table: default.test_virtual_db.t2
│       ├── scan id: 1
│       ├── output columns: [a (#2), v (#3), v['b']['c'] (#5)]
│       ├── read rows: 1
│       ├── read size: < 1 KiB
│       ├── partitions total: 1
│       ├── partitions scanned: 1
│       ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
│       ├── push downs: [filters: [is_true(TRY_CAST(t2.v['b']['c'] (#5) AS Int32 NULL) > t2.a (#2))], limit: NONE]
│       ├── virtual columns: [v['b']['c']]
│       └── estimated rows: 1.00
└── TableScan(Probe)
    ├── table: default.test_virtual_db.t1
    ├── scan id: 0
    ├── output columns: [a (#0), v (#1)]
    ├── read rows: 1
    ├── read size: < 1 KiB
    ├── partitions total: 1
    ├── partitions scanned: 1
    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
    ├── push downs: [filters: [], limit: NONE]
    └── estimated rows: 1.00

query T
explain select t2.v['b']['c'], sum(t1.a) from t2 join t1 on t2.v['b']['c'] = t1.a group by t2.v['b']['c'];
----
AggregateFinal
├── output columns: [sum(t1.a) (#6), t2.v['b']['c'] (#4)]
├── group by: [v['b']['c']]
├── aggregate functions: [sum(a)]
├── estimated rows: 1.00
└── AggregatePartial
    ├── group by: [v['b']['c']]
    ├── aggregate functions: [sum(a)]
    ├── estimated rows: 1.00
    └── HashJoin
        ├── output columns: [t2.v['b']['c'] (#4), t1.a (#2)]
        ├── join type: INNER
        ├── build keys: [t1.a (#2)]
        ├── probe keys: [CAST(t2.v['b']['c'] (#4) AS Int32 NULL)]
        ├── keys is null equal: [false]
        ├── filters: []
        ├── estimated rows: 1.00
        ├── TableScan(Build)
        │   ├── table: default.test_virtual_db.t1
        │   ├── scan id: 1
        │   ├── output columns: [a (#2)]
        │   ├── read rows: 1
        │   ├── read size: < 1 KiB
        │   ├── partitions total: 1
        │   ├── partitions scanned: 1
        │   ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
        │   ├── push downs: [filters: [], limit: NONE]
        │   └── estimated rows: 1.00
        └── TableScan(Probe)
            ├── table: default.test_virtual_db.t2
            ├── scan id: 0
            ├── output columns: [v['b']['c'] (#4)]
            ├── read rows: 1
            ├── read size: < 1 KiB
            ├── partitions total: 1
            ├── partitions scanned: 1
            ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
            ├── push downs: [filters: [], limit: NONE]
            ├── virtual columns: [v['b']['c']]
            └── estimated rows: 1.00


statement ok
CREATE OR REPLACE TABLE data_source_a (
    entity_id VARCHAR,
    source_id VARCHAR,
    metadata_object VARIANT,
    content_object VARIANT,
    refresh_time TIMESTAMP
);

statement ok
CREATE OR REPLACE TABLE config_table (
    entity_id VARCHAR,
    source_id VARCHAR,
    process_mode VARCHAR
);

statement ok
INSERT INTO data_source_a VALUES
('ENTITY1', 'SRC1', '{"type": "T1"}', '{"event_date": 1609459200000, "category_a": "CA1", "category_b": "CB1"}', CURRENT_TIMESTAMP());

statement ok
INSERT INTO config_table VALUES('ENTITY1', 'SRC1', 'standard_mode');

query T
EXPLAIN WITH processed_dates AS (
    SELECT
        a.entity_id,
        a.source_id,
        TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE AS event_date
    FROM
        data_source_a a
    JOIN
        config_table c
    ON
        c.entity_id = a.entity_id
        AND c.source_id = a.source_id
        AND c.process_mode = 'standard_mode'
    GROUP BY
        1, 2, 3
),
data_aggregation AS (
    SELECT
        a.entity_id,
        a.source_id,
        COALESCE(a.metadata_object:type::VARCHAR, 'Unknown') AS type_code,
        a.content_object:category_a::VARCHAR AS primary_category,
        a.content_object:category_b::VARCHAR AS secondary_category,
        p.event_date AS event_date
    FROM
        data_source_a a
    JOIN
        processed_dates p
    ON
        p.entity_id = a.entity_id
        AND p.source_id = a.source_id
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE = p.event_date
    WHERE
        a.content_object:category_a::VARCHAR IS NOT NULL
        AND a.content_object:category_b::VARCHAR IS NOT NULL
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE IS NOT NULL
    GROUP BY
        1, 2, 3, 4, 5, 6
)
SELECT * FROM data_aggregation;
----
EvalScalar
├── output columns: [a.entity_id (#0), a.source_id (#1), event_date (#15), type_code (#24), primary_category (#25), secondary_category (#26)]
├── expressions: [group_item (#21), group_item (#22), group_item (#23)]
├── estimated rows: 1.00
└── AggregateFinal
    ├── output columns: [a.entity_id (#0), a.source_id (#1), type_code (#21), primary_category (#22), secondary_category (#23), event_date (#15)]
    ├── group by: [entity_id, source_id, type_code, primary_category, secondary_category, event_date]
    ├── aggregate functions: []
    ├── estimated rows: 1.00
    └── AggregatePartial
        ├── group by: [entity_id, source_id, type_code, primary_category, secondary_category, event_date]
        ├── aggregate functions: []
        ├── estimated rows: 1.00
        └── EvalScalar
            ├── output columns: [event_date (#15), a.entity_id (#0), a.source_id (#1), type_code (#21), primary_category (#22), secondary_category (#23)]
            ├── expressions: [if(CAST(is_not_null(CAST(data_source_a.metadata_object['type'] (#18) AS String NULL)) AS Boolean NULL), CAST(assume_not_null(CAST(data_source_a.metadata_object['type'] (#18) AS String NULL)) AS String NULL), true, 'Unknown', NULL), CAST(data_source_a.content_object['category_a'] (#19) AS String NULL), CAST(data_source_a.content_object['category_b'] (#20) AS String NULL)]
            ├── estimated rows: 0.20
            └── HashJoin
                ├── output columns: [event_date (#15), a.metadata_object['type'] (#18), a.content_object['category_a'] (#19), a.content_object['category_b'] (#20), a.entity_id (#0), a.source_id (#1)]
                ├── join type: INNER
                ├── build keys: [a.entity_id (#0), a.source_id (#1), CAST(CAST(CAST(data_source_a.content_object['event_date'] (#16) AS Int64 NULL) AS Timestamp NULL) AS Date NULL)]
                ├── probe keys: [p.entity_id (#5), p.source_id (#6), p.event_date (#15)]
                ├── keys is null equal: [false, false, false]
                ├── filters: []
                ├── build join filters:
                │   ├── filter id:2, build key:a.entity_id (#0), probe targets:[a.entity_id (#5)@scan1, c.entity_id (#10)@scan2], filter type:bloom,inlist,min_max
                │   └── filter id:3, build key:a.source_id (#1), probe targets:[a.source_id (#6)@scan1, c.source_id (#11)@scan2], filter type:bloom,inlist,min_max
                ├── estimated rows: 0.20
                ├── Filter(Build)
                │   ├── output columns: [a.entity_id (#0), a.source_id (#1), a.content_object['event_date'] (#16), a.metadata_object['type'] (#18), a.content_object['category_a'] (#19), a.content_object['category_b'] (#20)]
                │   ├── filters: [is_not_null(CAST(data_source_a.content_object['category_a'] (#19) AS String NULL)), is_not_null(CAST(data_source_a.content_object['category_b'] (#20) AS String NULL)), is_not_null(CAST(CAST(CAST(data_source_a.content_object['event_date'] (#16) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))]
                │   ├── estimated rows: 0.20
                │   └── TableScan
                │       ├── table: default.test_virtual_db.data_source_a
                │       ├── scan id: 0
                │       ├── output columns: [entity_id (#0), source_id (#1), content_object['event_date'] (#16), metadata_object['type'] (#18), content_object['category_a'] (#19), content_object['category_b'] (#20)]
                │       ├── read rows: 1
                │       ├── read size: < 1 KiB
                │       ├── partitions total: 1
                │       ├── partitions scanned: 1
                │       ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
                │       ├── push downs: [filters: [is_not_null(CAST(data_source_a.content_object['category_a'] (#19) AS String NULL)) and is_not_null(CAST(data_source_a.content_object['category_b'] (#20) AS String NULL)) and is_not_null(CAST(CAST(CAST(data_source_a.content_object['event_date'] (#16) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))], limit: NONE]
                │       ├── virtual columns: [content_object['category_a'], content_object['category_b'], content_object['event_date'], metadata_object['type']]
                │       └── estimated rows: 1.00
                └── EvalScalar(Probe)
                    ├── output columns: [a.entity_id (#5), a.source_id (#6), event_date (#15)]
                    ├── expressions: [group_item (#14)]
                    ├── estimated rows: 1.00
                    └── AggregateFinal
                        ├── output columns: [a.entity_id (#5), a.source_id (#6), event_date (#14)]
                        ├── group by: [entity_id, source_id, event_date]
                        ├── aggregate functions: []
                        ├── estimated rows: 1.00
                        └── AggregatePartial
                            ├── group by: [entity_id, source_id, event_date]
                            ├── aggregate functions: []
                            ├── estimated rows: 1.00
                            └── EvalScalar
                                ├── output columns: [a.entity_id (#5), a.source_id (#6), event_date (#14)]
                                ├── expressions: [CAST(CAST(CAST(data_source_a.content_object['event_date'] (#13) AS Int64 NULL) AS Timestamp NULL) AS Date NULL)]
                                ├── estimated rows: 0.20
                                └── HashJoin
                                    ├── output columns: [a.content_object['event_date'] (#13), a.entity_id (#5), a.source_id (#6)]
                                    ├── join type: INNER
                                    ├── build keys: [a.entity_id (#5), a.source_id (#6)]
                                    ├── probe keys: [c.entity_id (#10), c.source_id (#11)]
                                    ├── keys is null equal: [false, false]
                                    ├── filters: []
                                    ├── build join filters:
                                    │   ├── filter id:0, build key:a.entity_id (#5), probe targets:[c.entity_id (#10)@scan2], filter type:bloom,inlist,min_max
                                    │   └── filter id:1, build key:a.source_id (#6), probe targets:[c.source_id (#11)@scan2], filter type:bloom,inlist,min_max
                                    ├── estimated rows: 0.20
                                    ├── Filter(Build)
                                    │   ├── output columns: [a.entity_id (#5), a.source_id (#6), a.content_object['event_date'] (#13)]
                                    │   ├── filters: [is_not_null(CAST(CAST(CAST(data_source_a.content_object['event_date'] (#13) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))]
                                    │   ├── estimated rows: 0.20
                                    │   └── TableScan
                                    │       ├── table: default.test_virtual_db.data_source_a
                                    │       ├── scan id: 1
                                    │       ├── output columns: [entity_id (#5), source_id (#6), content_object['event_date'] (#13)]
                                    │       ├── read rows: 1
                                    │       ├── read size: < 1 KiB
                                    │       ├── partitions total: 1
                                    │       ├── partitions scanned: 1
                                    │       ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
                                    │       ├── push downs: [filters: [is_not_null(CAST(CAST(CAST(data_source_a.content_object['event_date'] (#13) AS Int64 NULL) AS Timestamp NULL) AS Date NULL))], limit: NONE]
                                    │       ├── apply join filters: [#2, #3]
                                    │       ├── virtual columns: [content_object['event_date']]
                                    │       └── estimated rows: 1.00
                                    └── TableScan(Probe)
                                        ├── table: default.test_virtual_db.config_table
                                        ├── scan id: 2
                                        ├── output columns: [entity_id (#10), source_id (#11)]
                                        ├── read rows: 1
                                        ├── read size: < 1 KiB
                                        ├── partitions total: 1
                                        ├── partitions scanned: 1
                                        ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>, bloom pruning: 1 to 1 cost: <slt:ignore>>]
                                        ├── push downs: [filters: [is_true(config_table.process_mode (#12) = 'standard_mode')], limit: NONE]
                                        ├── apply join filters: [#2, #3, #0, #1]
                                        └── estimated rows: 1.00

query TTTTT?
WITH processed_dates AS (
    SELECT
        a.entity_id,
        a.source_id,
        TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE AS event_date
    FROM
        data_source_a a
    JOIN
        config_table c
    ON
        c.entity_id = a.entity_id
        AND c.source_id = a.source_id
        AND c.process_mode = 'standard_mode'
    GROUP BY
        1, 2, 3
),
data_aggregation AS (
    SELECT
        a.entity_id,
        a.source_id,
        COALESCE(a.metadata_object:type::VARCHAR, 'Unknown') AS type_code,
        a.content_object:category_a::VARCHAR AS primary_category,
        a.content_object:category_b::VARCHAR AS secondary_category,
        p.event_date AS event_date
    FROM
        data_source_a a
    JOIN
        processed_dates p
    ON
        p.entity_id = a.entity_id
        AND p.source_id = a.source_id
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE = p.event_date
    WHERE
        a.content_object:category_a::VARCHAR IS NOT NULL
        AND a.content_object:category_b::VARCHAR IS NOT NULL
        AND TO_TIMESTAMP(a.content_object:event_date::BIGINT)::DATE IS NOT NULL
    GROUP BY
        1, 2, 3, 4, 5, 6
)
SELECT * FROM data_aggregation;
----
ENTITY1 SRC1 T1 CA1 CB1 2021-01-01

statement ok
CREATE OR REPLACE TABLE data_main (
    record_id VARCHAR,
    category_id VARCHAR,
    data_object VARIANT
);

statement ok
CREATE OR REPLACE TABLE data_staging (
    data_object VARIANT
);

statement ok
INSERT INTO data_main (record_id, category_id, data_object) VALUES
('rec1', 'cat1', '{"timestamp": 1625000000000, "metadata": {"category_id": "cat1", "timestamp": 1625000000000}}');

statement ok
INSERT INTO data_staging (data_object) VALUES
('{"unique_key": "rec1", "metadata": {"category_id": "cat1"}, "timestamp": 1625100000000}');

query T
EXPLAIN MERGE INTO data_main target
USING (
  SELECT
    data_object:unique_key AS record_id,
    data_object:metadata.category_id AS category_id,
    1624900000000 AS reference_time
  FROM data_staging
) source
ON target.record_id = source.record_id AND target.category_id = source.category_id
WHEN MATCHED THEN
UPDATE SET
  data_object = json_object_insert(
    target.data_object,
    'metadata',
    json_object_insert(
      json_object_insert(
        target.data_object:metadata,
        'reference_time',
        source.reference_time::variant,
        true
      ),
      'time_difference',
      coalesce((target.data_object:metadata.timestamp - source.reference_time) / 24 / 60 / 60 / 1000::variant, null::variant),
      true
    ),
    true
  );
----
CommitSink
└── DataMutation
    ├── target table: default.test_virtual_db.data_main
    └── MutationManipulate
        ├── matched update: [condition: None, update set data_object = if(CAST(_predicate (#18446744073709551615) AS Boolean NULL), object_insert(target.data_object (#6), 'metadata', object_insert(object_insert(get_by_keypath(target.data_object (#6), '{"metadata"}'), 'reference_time', CAST(source.reference_time (#3) AS Variant), true), 'time_difference', if(CAST(is_not_null((TRY_CAST(get_by_keypath(target.data_object (#6), '{"metadata","timestamp"}') AS UInt8 NULL) - CAST(source.reference_time (#3) AS UInt64 NULL)) / 24 / 60 / 60 / NULL) AS Boolean NULL), CAST(assume_not_null((TRY_CAST(get_by_keypath(target.data_object (#6), '{"metadata","timestamp"}') AS UInt8 NULL) - CAST(source.reference_time (#3) AS UInt64 NULL)) / 24 / 60 / 60 / NULL) AS Float64 NULL), NULL), true), true), target.data_object (#6))]
        └── RowFetch
            ├── output columns: [target.record_id (#4), target.category_id (#5), target._row_id (#7), data_staging.data_object['unique_key'] (#1), data_staging.data_object['metadata']['category_id'] (#2), reference_time (#3), target.data_object (#6)]
            ├── columns to fetch: [data_object]
            └── HashJoin
                ├── output columns: [target.record_id (#4), target.category_id (#5), target._row_id (#7), data_staging.data_object['unique_key'] (#1), data_staging.data_object['metadata']['category_id'] (#2), reference_time (#3)]
                ├── join type: INNER
                ├── build keys: [CAST(source.record_id (#1) AS String NULL), CAST(source.category_id (#2) AS String NULL)]
                ├── probe keys: [target.record_id (#4), target.category_id (#5)]
                ├── keys is null equal: [false, false]
                ├── filters: []
                ├── build join filters:
                │   ├── filter id:0, build key:CAST(source.record_id (#1) AS String NULL), probe targets:[target.record_id (#4)@scan1], filter type:bloom,inlist,min_max
                │   └── filter id:1, build key:CAST(source.category_id (#2) AS String NULL), probe targets:[target.category_id (#5)@scan1], filter type:bloom,inlist,min_max
                ├── estimated rows: 1.00
                ├── EvalScalar(Build)
                │   ├── output columns: [data_staging.data_object['unique_key'] (#1), data_staging.data_object['metadata']['category_id'] (#2), reference_time (#3)]
                │   ├── expressions: [1624900000000]
                │   ├── estimated rows: 1.00
                │   └── TableScan
                │       ├── table: default.test_virtual_db.data_staging
                │       ├── scan id: 0
                │       ├── output columns: [data_object['unique_key'] (#1), data_object['metadata']['category_id'] (#2)]
                │       ├── read rows: 1
                │       ├── read size: < 1 KiB
                │       ├── partitions total: 1
                │       ├── partitions scanned: 1
                │       ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
                │       ├── push downs: [filters: [], limit: NONE]
                │       ├── virtual columns: [data_object['metadata']['category_id'], data_object['unique_key']]
                │       └── estimated rows: 1.00
                └── TableScan(Probe)
                    ├── table: default.test_virtual_db.data_main
                    ├── scan id: 1
                    ├── output columns: [record_id (#4), category_id (#5), _row_id (#7)]
                    ├── read rows: 1
                    ├── read size: < 1 KiB
                    ├── partitions total: 1
                    ├── partitions scanned: 1
                    ├── pruning stats: [segments: <range pruning: 1 to 1 cost: <slt:ignore>>, blocks: <range pruning: 1 to 1 cost: <slt:ignore>>]
                    ├── push downs: [filters: [], limit: NONE]
                    ├── apply join filters: [#0, #1]
                    └── estimated rows: 1.00

query TT?
SELECT * FROM data_main;
----
rec1 cat1 {"metadata":{"category_id":"cat1","timestamp":1625000000000},"timestamp":1625000000000}

statement ok
drop table t1

statement ok
drop table t2

statement ok
drop table data_source_a

statement ok
drop table config_table

statement ok
drop table data_main

statement ok
drop table data_staging

statement ok
set enable_experimental_virtual_column = 0;

statement ok
USE default

statement ok
DROP DATABASE IF EXISTS test_virtual_db
