# Usage:
# databend-query -c databend-query.toml

[query]
max_active_sessions = 256
shutdown_wait_timeout_ms = 5000

# Internal flight rpc for cluster communication.
flight_api_address = "0.0.0.0:9090"

# Admin REST API.
admin_api_address = "0.0.0.0:8080"

# Metrics REST API.
metric_api_address = "0.0.0.0:7070"

# Query Handler: MySQL
mysql_handler_host = "0.0.0.0"
mysql_handler_port = 3307

# Query Handler: Clickhouse HTTP
clickhouse_http_handler_host = "0.0.0.0"
clickhouse_http_handler_port = 8124

# Query Handler: HTTP API
http_handler_host = "0.0.0.0"
http_handler_port = 8000

# Query Handler: Experimental Arrow Flight SQL API
flight_sql_handler_host = "0.0.0.0"
flight_sql_handler_port = 8900

tenant_id = "default"
cluster_id = "default"

table_engine_memory_enabled = true

# [[query.users]]
# name = "root"
# auth_type = "no_password"

# [[query.users]]
# name = "databend"
# auth_type = "double_sha1_password"
# # to generate auth_string: echo -n "databend" | sha1sum | cut -d' ' -f1 | xxd -r -p | sha1sum
# auth_string = "3081f32caef285c232d066033c89a78d88a6d8a5"

# [[query.users]]
# name = "datafuselabs"
# auth_type = "sha256_password"
# # to generate auth_string: echo -n "datafuselabs" | sha256sum
# auth_string = "6db1a2f5da402b43c066fcadcbf78f04260b3236d9035e44dd463f21e29e6f3b"


[log]

[log.file]
level = "WARN"
format = "text"
dir = "/var/log/databend"

[meta]
# It is a list of `grpc_api_advertise_host:<grpc-api-port>` of databend-meta config
endpoints = ["0.0.0.0:9191"]
username = "root"
password = "root"
client_timeout_in_second = 60
auto_sync_interval = 60

# Storage config.
[storage]
# fs | s3 | azblob | gcs | oss | cos | hdfs | webhdfs
type = "fs"

# Set a local folder to store your data.
# Comment out this block if you're NOT using local file system as storage.
[storage.fs]
data_path = "/var/lib/databend/data"

# To use an Amazon S3-like storage service, uncomment this block and set your values.
# [storage.s3]
# bucket = "<your-bucket-name>"
# endpoint_url = "<your-endpoint>"
# access_key_id = "<your-key-id>"
# secret_access_key = "<your-account-key>"
# enable_virtual_host_style = false

# To use Azure Blob Storage, uncomment this block and set your values.
# [storage.azblob]
# endpoint_url = "https://<your-storage-account-name>.blob.core.windows.net"
# container = "<your-azure-storage-container-name>"
# account_name = "<your-storage-account-name>"
# account_key = "<your-account-key>"

# To use Google Cloud Storage, uncomment this block and set your values.
# [storage.gcs]
# bucket = "<your-bucket-name>"
# endpoint_url = "<your-endpoint>"
# credential = "<your-credentials>"

# To use Alibaba Cloud OSS, uncomment this block and set your values.
# [storage.oss]
# bucket = "<your-bucket-name>"
# endpoint_url = "<your-endpoint>"
# access_key_id = "<your-key-id>"
# access_key_secret = "<your-account-key>"

# To use Tencent Cloud Object Storage, uncomment this block and set your values.
# [storage.cos]
# bucket = "<your-bucket-name>"
# endpoint_url = "<your-endpoint>"
# secret_id = "<your-secret-id>"
# secret_key = "<your-secret-key>"

# To use HDFS, uncomment this block and set your values.
# [storage.hdfs]
# name_node = "<your-name-node>"

# To use WebHDFS, uncomment this block and set your values.
# [storage.webhdfs]
# endpoint_url = "<your-endpoint>"

# Cache config.
[cache]
# Type of storage to keep the table data cache
#
# available options: [none|disk]
# default is "none", which disable table data cache
# use "disk" to enabled disk cache
data_cache_storage = "none"

# Policy of data cache key reloading
#
# Available options: [reset|fuzzy]
# "reset":  remove previous data cache during restart  (default value)
# "fuzzy":  reload cache keys from cache dir, retaining the cache data
#           that existed before the restart

# data_cache_key_reload_policy = "fuzzy"

[cache.disk]
# cache path
path = "/var/lib/databend/cache"
# max bytes of cached data 20G
max_bytes = 21474836480
